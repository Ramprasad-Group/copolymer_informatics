{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "## Prepare data set: split, scaler, and create tensorflow data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Read the data set\n",
    "# df = pd.read_pickle('pkls/dataset.pkl')\n",
    "\n",
    "df = df.sample(frac=1, random_state=123)\n",
    "\n",
    "df['scaled_value'] = df['val']\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "props = df.prop.unique().tolist()\n",
    "\n",
    "datasets = []\n",
    "def get_dataset(df):\n",
    "    # return TF data set\n",
    "    fps_mix = np.stack(df.fps_mix.values).astype(np.float32)\n",
    "    selector = np.stack(df.dummy.values).astype(np.float32)   \n",
    "    target = df.scaled_value.astype(np.float32)[:, np.newaxis]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({'sel': selector, 'fps_mix': fps_mix, 'prop': df.prop}, target))\n",
    "\n",
    "    dataset = dataset.cache().batch(200).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "training_df, test_df = train_test_split(df, test_size=0.2, stratify=df.prop, random_state=123)\n",
    "training_df, test_df = training_df.copy(), test_df.copy()\n",
    "\n",
    "for train_index, val_index in skf.split(training_df, training_df.prop):\n",
    "    # iterte over 5 splits\n",
    "    train_df = training_df.iloc[train_index].copy()\n",
    "    val_df = training_df.iloc[val_index].copy()\n",
    "    \n",
    "    # scale target values\n",
    "    property_scaler = {}\n",
    "    for prop in props:\n",
    "        property_scaler[prop] = MinMaxScaler()\n",
    "\n",
    "        # train\n",
    "        cond = train_df[train_df.prop == prop].index\n",
    "        train_df.loc[cond, ['scaled_value']] = property_scaler[prop].fit_transform(train_df.loc[cond, ['scaled_value']])\n",
    "        \n",
    "        # val\n",
    "        cond = val_df[val_df.prop == prop].index\n",
    "        val_df.loc[cond, ['scaled_value']] = property_scaler[prop].transform(val_df.loc[cond, ['scaled_value']])\n",
    "\n",
    "    datasets.append({'train': get_dataset(train_df), 'val': get_dataset(val_df), 'property_scaler': property_scaler})\n",
    "\n",
    "# Create final dataset for meta learner\n",
    "property_scaler_final = {}\n",
    "for prop in props:\n",
    "    property_scaler_final[prop] = MinMaxScaler()\n",
    "    \n",
    "   # train\n",
    "    cond = training_df[training_df.prop == prop].index\n",
    "    training_df.loc[cond, ['scaled_value']] = property_scaler_final[prop].fit_transform(training_df.loc[cond, ['scaled_value']])\n",
    "\n",
    "    # val\n",
    "    cond = test_df[test_df.prop == prop].index\n",
    "    test_df.loc[cond, ['scaled_value']] = property_scaler_final[prop].transform(test_df.loc[cond, ['scaled_value']])\n",
    "    \n",
    "datasets_final = {'train': get_dataset(training_df), 'test': get_dataset(test_df), 'property_scaler': property_scaler_final}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk\n",
    "import tensorflow as tf\n",
    "from datetime import datetime \n",
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import Hyperband\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "class PropertyDonwsteam(tfk.Model):\n",
    "    def __init__(self, hp):\n",
    "        super().__init__()\n",
    "        # hp defines the hyper parameter\n",
    "        self.my_layers = []\n",
    "        self.concat_at = hp.Int('concat_at', 0, 2)\n",
    "        \n",
    "        for i in range(hp.Int('num_layers', 3, 3)): \n",
    "            new_step = [               \n",
    "            tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=352,\n",
    "                                            max_value=544,\n",
    "                                            step=64),),\n",
    "            \n",
    "            tf.keras.layers.PReLU(),\n",
    "            tf.keras.layers.Dropout(hp.Float(\n",
    "                'dropout_' + str(i),\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.1,\n",
    "            )),\n",
    "            ]\n",
    "\n",
    "            self.my_layers.append(new_step)\n",
    "        self.my_layers.append([tf.keras.layers.Dense(1)])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs['fps_mix']\n",
    "        for num, layer_step in enumerate(self.my_layers):\n",
    "            if self.concat_at == num:\n",
    "                # concatenate the selector vector\n",
    "                x = tf.concat((x, inputs['sel']), -1)\n",
    "            for layer in layer_step:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def predict_step(self, data):\n",
    "        data = data_adapter.expand_1d(data)\n",
    "        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n",
    "\n",
    "        # drop prop here\n",
    "        prop = x['prop']\n",
    "        del x['prop']\n",
    "        return self(x, training=False), data[-1], prop\n",
    "\n",
    "    @tf.function\n",
    "    def call_external(self, inputs):\n",
    "        return self.call(inputs)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    # returns the compiled tensorflow model\n",
    "    model = PropertyDonwsteam(hp)\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-3]))\n",
    "    opt = tfa.optimizers.SWA(opt)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='mse',)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all five CV models and optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "results, property_metric, best_values = [], [], []\n",
    "\n",
    "for num, data in enumerate(datasets):\n",
    "    # iterate over all 5 VC data sets    \n",
    "    tuner = Hyperband(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_epochs=300,\n",
    "        seed=10,\n",
    "        directory=f'hyperparameter_search_fp',\n",
    "        project_name='fold_' + str(num)\n",
    "        )\n",
    "\n",
    "    reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.8,\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=40)\n",
    "    \n",
    "    class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "        def on_train_end(*args, **kwargs):\n",
    "            IPython.display.clear_output(wait = True)\n",
    "    \n",
    "    # Create an instance of the model and search the hyperspace\n",
    "    tuner.search(data['train'],\n",
    "                epochs=300,\n",
    "                validation_data=data['val'],\n",
    "                callbacks=[earlystop, reduce_lr, ClearTrainingOutput()],\n",
    "                verbose=0\n",
    "                )\n",
    "    \n",
    "    ## Post processing: compute RMSE and R2 values (scale back before comnputation)\n",
    "    best_values.append(tuner.get_best_hyperparameters()[0].values)\n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "    # Predict on the validataion data set\n",
    "    res = np.concatenate(best_model.predict(data['val']), -1)\n",
    "    \n",
    "    # Save best model for later use in the meta learner\n",
    "    best_model.save(f'models/fp/{num}', include_optimizer=False)\n",
    "\n",
    "    # Compute RMSE and R2\n",
    "    _df = pd.DataFrame(res, columns=['pred', 'target', 'prop'])\n",
    "    _df['prop'] = _df.prop.apply(lambda x: x.decode('utf-8'))\n",
    "    props = _df.prop.unique()\n",
    "    \n",
    "    property_scaler = data['property_scaler']\n",
    "    for prop in props:\n",
    "\n",
    "        cond = _df[_df.prop == prop].index\n",
    "        rmse_scaled = mean_squared_error(_df.loc[cond, ['target']], _df.loc[cond, ['pred']], squared=False)\n",
    "        r2_scaled = r2_score(_df.loc[cond, ['target']], _df.loc[cond, ['pred']])\n",
    "        \n",
    "        _df.loc[cond, ['pred']] = property_scaler[prop].inverse_transform(_df.loc[cond, ['pred']])\n",
    "        _df.loc[cond, ['target']] = property_scaler[prop].inverse_transform(_df.loc[cond, ['target']])\n",
    "        \n",
    "        rmse = mean_squared_error(_df.loc[cond, ['target']], _df.loc[cond, ['pred']], squared=False)\n",
    "        r2 = r2_score(_df.loc[cond, ['target']], _df.loc[cond, ['pred']])\n",
    "        property_metric.append({'name': f'fp', 'prop': prop, 'rmse': rmse, 'r2':r2, 'fold': num, 'rmse_scaled': rmse_scaled, 'r2_scaled': r2_scaled})\n",
    "        \n",
    "    # Not scaled back\n",
    "    rmse = mean_squared_error(res[:,0], res[:,1], squared=False)\n",
    "    r2 = r2_score(res[:,0], res[:,1])\n",
    "    \n",
    "    results.append({'name': f'fp','r2': r2, 'rmse':rmse})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit (conda)",
   "name": "python3610jvsc74a57bd05a23ace4a35b71da36bcef69b0e3fc89ef4f6105456eb3826155e74e20a4e06f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "f38aa9696910a9ab861a01146cd947ddff24dfeeb60415d6a09117143e516b0e"
   }
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}